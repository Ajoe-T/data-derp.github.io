---
sidebar_position: 5
---

# Data Formats
## Wait...what’s data serialization again?
(extract from [Devopedia](https://devopedia.org/data-serialization))

* Data serialization is the process of converting data objects present in complex data structures into a byte stream for storage, transfer and distribution purposes on physical devices.

* Computer systems may **vary** in their hardware architecture, OS, addressing mechanisms. Internal representations of data also vary accordingly in every environment/language. Storing and exchanging data between such varying environments requires a **platform-and-language-neutral data format** that all systems understand.

* Once the serialized data is transmitted from the source machine to the destination machine, the reverse process of creating objects from the byte sequence called **deserialization** is carried out. Reconstructed objects are clones of the original object.


<div style={{textAlign: 'center'}}>

![data-serialisation.png](./assets/data-serialisation.png)

</div>

## Data Formats

<div style={{textAlign: 'center'}}>
    <figure class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/jKfKmBdPuT4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </figure>
Avro vs. Parquet
</div>

Read: [Big Data File Formats](https://luminousmen.com/post/big-data-file-formats)

Read: [Comparing the two formats](https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/#:~:text=The%20biggest%20difference%20between%20ORC,in%20a%20row%2Dbased%20format.&text=While%20column%2Doriented%20stores%20like,might%20be%20the%20better%20choice.)
* [Example with Python](https://www.confessionsofadataguy.com/big-data-file-showdown-avro-vs-parquet-with-python/)
* We’ll cover Parquet quite extensively, but here is where you’ll often find Avro:
  * [Popular choice for streaming](https://www.confluent.io/blog/avro-kafka-data/) and persisting streaming data into data lakes (e.g. [Azure Event Hubs Capture](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#exploring-the-captured-files-and-working-with-avro))

<div style={{textAlign: 'center'}}>

<figure class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/1j8SdS7s_NY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>

A Deeper Dive

</div>

## Apache Parquet Recap

<div style={{textAlign: 'center'}}>

![parquet-columnar-storage.png](./assets/parquet-columnar-storage.png)

![parquet-columnar-storage-for-the-people.png](./assets/parquet-columnar-storage-for-the-people.png)

![parquet-access-only-data-you-need.png](./assets/parquet-access-only-data-you-need.png)

</div>

## Apache Avro Recap
The only things you need to take away are that Avro files:
* are self-describing (schema accompanies data)
* are **row-oriented**
* support schema evolution
* are a popular serialization format for **message streams**

<div style={{textAlign: 'center'}}>

![avro-recap.png](./assets/avro-recap.png)

</div>

## Check Your Learning!
Unlike MapReduce vs Spark, there’s no clear winner.
There’s always still a time and place for each of these formats!

| | CSV | JSON | Parquet | Avro |
| --- | --- | --- | --- | --- |
|Compressibility | :white_check_mark: | :white_check_mark: | | |
|Human Readability | :white_check_mark: | :white_check_mark: | | |
|Schema Evolution | | | | :white_check_mark: |
|Human Readability | | | :white_check_mark: | :white_check_mark: |