---
sidebar_position: 4
---
# Exercise: Create Infrastructure in the Cloud

<div style={{textAlign: 'center'}}>

<figure class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/_EQqXvdcxJY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>
</div>


Now that we've created artifacts (`main.py`, `data_ingestion-0.1-py3.egg`) in our S3 bucket, our next step is to run that logic in AWS Glue. In this exercise, we'll learn how to create AWS Glue Jobs and Crawlers and how to view the resulting data in AWS Athena.

Follow the instructions [here](https://github.com/data-derp/exercise-co2-vs-temperature-infrastructure/blob/master/data-ingestion.md) to get started.

<div style={{textAlign: 'center'}}>

![project-structure-ingestion-navi.png](./assets/project-structure-ingestion-navi.png)

</div>
